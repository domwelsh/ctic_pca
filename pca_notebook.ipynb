{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) and Multiple Factor Analysis (MFA) Tutorial\n",
    "\n",
    "This notebook explores dimensionality reduction techniques using tumor diagnosis data. We'll cover the theoretical concepts and practical implementation of both PCA and MFA.\n",
    "\n",
    "## Contents\n",
    "1. [Data Loading and Exploration](#data)\n",
    "2. [Principal Component Analysis (PCA)](#pca)\n",
    "   - Theory and Concepts\n",
    "   - Implementation\n",
    "   - Visualization and Interpretation\n",
    "3. [Multiple Factor Analysis (MFA)](#mfa)\n",
    "   - Theory and Concepts\n",
    "   - Implementation\n",
    "   - Visualization and Interpretation\n",
    "4. [Comparison and Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration\n",
    "Let's start by loading the tumor diagnosis dataset and exploring its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tumor diagnosis data\n",
    "df = pd.read_excel('tumour_diagnosis.xlsx')\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset structure\n",
    "print(\"Column names:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"Number of features: {len(df.columns)-1}\")  # Assuming one column is the target\n",
    "print(\"Data types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum().sum())\n",
    "print(\"Basic statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Principal Component Analysis (PCA)\n",
    "\n",
    "### Theory and Concepts\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while preserving as much variance as possible.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "**1. Principal Components**: These are new variables that are linear combinations of the original features. They are ordered by the amount of variance they explain in the data.\n",
    "\n",
    "**2. Eigenvalues and Eigenvectors**: \n",
    "- **Eigenvectors** determine the direction of the principal components\n",
    "- **Eigenvalues** represent the amount of variance explained by each component\n",
    "\n",
    "**3. Variance Explained**: Each principal component explains a certain percentage of the total variance in the dataset.\n",
    "\n",
    "#### Mathematical Foundation:\n",
    "\n",
    "For a data matrix X (n × p), PCA finds directions (principal components) along which the data varies the most:\n",
    "\n",
    "1. **Standardize the data**: X_standardized = (X - μ) / σ\n",
    "2. **Compute covariance matrix**: C = (1/(n-1)) * X^T * X\n",
    "3. **Find eigenvalues and eigenvectors**: C * v = λ * v\n",
    "4. **Transform data**: Y = X * V (where V is the matrix of eigenvectors)\n",
    "\n",
    "#### When to use PCA:\n",
    "- High-dimensional data with potential redundancy\n",
    "- Visualization of complex datasets\n",
    "- Noise reduction\n",
    "- Feature extraction for machine learning\n",
    "- Data compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Implementation\n",
    "\n",
    "Let's prepare our data and apply PCA to the tumor diagnosis dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PCA\n",
    "# Identify the target column (usually the first or last column for diagnosis data)\n",
    "# Let's assume the first column is ID and second is diagnosis\n",
    "if 'diagnosis' in df.columns:\n",
    "    target_col = 'diagnosis'\n",
    "elif 'Diagnosis' in df.columns:\n",
    "    target_col = 'Diagnosis'\n",
    "else:\n",
    "    # Take the second column as target if no diagnosis column found\n",
    "    target_col = df.columns[1]\n",
    "\n",
    "print(f\"Target column: {target_col}\")\n",
    "print(f\"Target values: {df[target_col].unique()}\")\n",
    "\n",
    "# Separate features and target, excluding ID columns\n",
    "# Get numeric columns and remove target and ID columns\n",
    "X = df.select_dtypes(include=[np.number])\n",
    "columns_to_exclude = [target_col]\n",
    "\n",
    "# Identify and exclude ID columns (common patterns)\n",
    "id_columns = [col for col in X.columns if 'id' in col.lower() or 'ID' in col or col.lower().startswith('id_')]\n",
    "columns_to_exclude.extend(id_columns)\n",
    "\n",
    "# Remove excluded columns\n",
    "X = X.drop(columns=columns_to_exclude, errors='ignore')\n",
    "\n",
    "# Get target variable\n",
    "y = df[target_col] if target_col in df.columns else None\n",
    "\n",
    "print(f\"\\nExcluded columns: {columns_to_exclude}\")\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Features: {X.columns.tolist()[:5]}...\")  # Show first 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Data standardized. Mean should be ~0, std should be ~1:\")\n",
    "print(f\"Mean: {np.mean(X_scaled, axis=0)[:5]}\")\n",
    "print(f\"Std: {np.std(X_scaled, axis=0)[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Original data shape: {X.shape}\")\n",
    "print(f\"PCA transformed shape: {X_pca.shape}\")\n",
    "print(f\"Explained variance ratio (first 10 components):\")\n",
    "print(pca.explained_variance_ratio_[:10])\n",
    "print(f\"Cumulative explained variance (first 10 components):\")\n",
    "print(np.cumsum(pca.explained_variance_ratio_)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Visualization and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot explained variance\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Scree plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, min(21, len(pca.explained_variance_ratio_)+1)),\n",
    "    pca.explained_variance_ratio_[:20], 'bo-')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot')\n",
    "plt.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D PCA visualization\n",
    "if y is not None:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create scatter plot colored by target\n",
    "    unique_labels = np.unique(y)\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = y == label\n",
    "        plt.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                   c=[colors[i]], label=f'{label}', alpha=0.6, s=50)\n",
    "    \n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    plt.title('PCA: First Two Principal Components')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    # If no target variable, just plot the points\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6)\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    plt.title('PCA: First Two Principal Components')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### PCA Biplot - Feature Contribution Visualization\n\nA **biplot** is what you're thinking of when you want to see \"features as lines\" on the PCA plot. It combines:\n\n1. **Observations** (data points) as dots\n2. **Feature loadings** (contributions) as arrows/vectors\n\nThe arrows show:\n- **Direction**: How each feature contributes to PC1 and PC2\n- **Length**: The magnitude of contribution \n- **Angle**: Features pointing in similar directions are correlated\n\n**Interpretation**:\n- Arrows pointing in the same direction represent positively correlated features\n- Arrows pointing in opposite directions represent negatively correlated features  \n- Longer arrows indicate features with stronger influence on the principal components\n- The angle between an arrow and a PC axis shows how much that feature contributes to that PC",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# PCA Biplot - showing both observations and feature contributions\ndef create_biplot(X_pca, pca, feature_names, y=None, pc1=0, pc2=1):\n    \"\"\"\n    Create a biplot showing both PCA scores and loadings\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(12, 8))\n    \n    # Plot observations (scores)\n    if y is not None:\n        unique_labels = np.unique(y)\n        colors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))\n        \n        for i, label in enumerate(unique_labels):\n            mask = y == label\n            ax.scatter(X_pca[mask, pc1], X_pca[mask, pc2], \n                      c=[colors[i]], label=f'{label}', alpha=0.6, s=50)\n    else:\n        ax.scatter(X_pca[:, pc1], X_pca[:, pc2], alpha=0.6, s=50)\n    \n    # Plot loadings (feature contributions) as arrows\n    loadings = pca.components_[[pc1, pc2]].T  # Shape: (n_features, 2)\n    \n    # Scale loadings to fit nicely with the scores\n    scale_factor = 0.7 * max(np.abs(X_pca[:, [pc1, pc2]]).max(), np.abs(loadings).max())\n    loadings_scaled = loadings * scale_factor\n    \n    # Only show top contributing features to avoid clutter\n    loading_magnitudes = np.sqrt(loadings[:, 0]**2 + loadings[:, 1]**2)\n    top_features_idx = np.argsort(loading_magnitudes)[-10:]  # Top 10 features\n    \n    for i in top_features_idx:\n        ax.arrow(0, 0, loadings_scaled[i, 0], loadings_scaled[i, 1],\n                head_width=0.05*scale_factor, head_length=0.05*scale_factor,\n                fc='red', ec='red', alpha=0.7)\n        \n        # Add feature labels\n        ax.text(loadings_scaled[i, 0]*1.1, loadings_scaled[i, 1]*1.1,\n               feature_names[i], fontsize=8, ha='center', va='center',\n               bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.7))\n    \n    ax.set_xlabel(f'PC{pc1+1} ({pca.explained_variance_ratio_[pc1]:.1%} variance)')\n    ax.set_ylabel(f'PC{pc2+1} ({pca.explained_variance_ratio_[pc2]:.1%} variance)')\n    ax.set_title('PCA Biplot: Observations and Feature Contributions')\n    \n    if y is not None:\n        ax.legend(loc='upper right')\n    \n    ax.grid(True, alpha=0.3)\n    ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n    ax.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n    \n    return fig, ax\n\n# Create the biplot\nfig, ax = create_biplot(X_pca, pca, X.columns.tolist(), y)\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multiple Factor Analysis (MFA)\n",
    "\n",
    "### Theory and Concepts\n",
    "\n",
    "Multiple Factor Analysis (MFA) is an extension of Principal Component Analysis designed to handle datasets with multiple groups of variables. It's particularly useful when dealing with mixed-type data or when variables can be naturally grouped.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "**1. Variable Groups**: MFA assumes that variables can be grouped into meaningful categories (e.g., morphological features, texture features, etc.).\n",
    "\n",
    "**2. Weighted PCA**: Each group of variables is first analyzed separately using PCA, then combined using weights that balance the contribution of each group.\n",
    "\n",
    "**3. Global Analysis**: After analyzing each group separately, MFA performs a global analysis that takes into account the structure revealed by each group.\n",
    "\n",
    "#### Mathematical Foundation:\n",
    "\n",
    "1. **Group-wise PCA**: For each group j, perform PCA and retain the first αⱼ components\n",
    "2. **Weight Calculation**: Weight each group by wⱼ = 1/λ₁ⱼ (where λ₁ⱼ is the first eigenvalue of group j)\n",
    "3. **Global Analysis**: Perform PCA on the weighted and concatenated data\n",
    "\n",
    "#### Advantages of MFA over PCA:\n",
    "- Handles grouped variables effectively\n",
    "- Prevents one group from dominating the analysis\n",
    "- Provides interpretation at both group and global levels\n",
    "- Better suited for mixed-type data\n",
    "\n",
    "#### When to use MFA:\n",
    "- Variables can be naturally grouped\n",
    "- Some groups have many more variables than others\n",
    "- You want to understand both group-specific and global patterns\n",
    "- Working with mixed data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFA Implementation\n",
    "\n",
    "Since scikit-learn doesn't have a direct MFA implementation, we'll create a simplified version that demonstrates the key concepts. For tumor diagnosis data, we might group features by their types (e.g., mean values, standard errors, worst values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature groups for MFA\n",
    "# For typical tumor data, features often come in groups (mean, SE, worst)\n",
    "feature_names = X.columns.tolist()\n",
    "print(\"Feature names sample:\", feature_names[:10])\n",
    "\n",
    "# Try to identify groups based on naming patterns\n",
    "groups = {}\n",
    "if any('_mean' in col or col.endswith('_m') for col in feature_names):\n",
    "    groups['mean'] = [col for col in feature_names if '_mean' in col or col.endswith('_m')]\n",
    "if any('_se' in col or 'se_' in col for col in feature_names):\n",
    "    groups['se'] = [col for col in feature_names if '_se' in col or 'se_' in col]\n",
    "if any('_worst' in col or col.endswith('_w') for col in feature_names):\n",
    "    groups['worst'] = [col for col in feature_names if '_worst' in col or col.endswith('_w')]\n",
    "\n",
    "# If no clear naming pattern, create groups by feature index\n",
    "if not groups:\n",
    "    n_features = len(feature_names)\n",
    "    group_size = n_features // 3\n",
    "    groups = {\n",
    "        'group1': feature_names[:group_size],\n",
    "        'group2': feature_names[group_size:2*group_size],\n",
    "        'group3': feature_names[2*group_size:]\n",
    "    }\n",
    "\n",
    "print(\"\\nFeature groups:\")\n",
    "for group_name, features in groups.items():\n",
    "    print(f\"{group_name}: {len(features)} features\")\n",
    "    print(f\"  Sample features: {features[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement simplified MFA\n",
    "class SimpleMFA:\n",
    "    def __init__(self, groups, n_components=2):\n",
    "        self.groups = groups\n",
    "        self.n_components = n_components\n",
    "        self.group_pcas = {}\n",
    "        self.group_weights = {}\n",
    "        self.global_pca = None\n",
    "        \n",
    "    def fit_transform(self, X):\n",
    "        # Step 1: Perform PCA on each group\n",
    "        group_scores = []\n",
    "        \n",
    "        for group_name, feature_list in self.groups.items():\n",
    "            # Get data for this group\n",
    "            group_data = X[feature_list]\n",
    "            \n",
    "            # Standardize group data\n",
    "            scaler = StandardScaler()\n",
    "            group_scaled = scaler.fit_transform(group_data)\n",
    "            \n",
    "            # Apply PCA to group\n",
    "            pca = PCA()\n",
    "            group_pca_scores = pca.fit_transform(group_scaled)\n",
    "            \n",
    "            # Store PCA object and calculate weight\n",
    "            self.group_pcas[group_name] = pca\n",
    "            self.group_weights[group_name] = 1.0 / pca.explained_variance_[0]  # Weight by first eigenvalue\n",
    "            \n",
    "            # Keep all components for now\n",
    "            group_scores.append(group_pca_scores)\n",
    "            \n",
    "            print(f\"Group {group_name}:\")\n",
    "            print(f\"  First eigenvalue: {pca.explained_variance_[0]:.3f}\")\n",
    "            print(f\"  Weight: {self.group_weights[group_name]:.3f}\")\n",
    "            print(f\"  Explained variance (first 3 components): {pca.explained_variance_ratio_[:3]}\")\n",
    "        \n",
    "        # Step 2: Combine weighted group scores\n",
    "        weighted_scores = []\n",
    "        for i, (group_name, scores) in enumerate(zip(self.groups.keys(), group_scores)):\n",
    "            weight = self.group_weights[group_name]\n",
    "            weighted_scores.append(scores * np.sqrt(weight))  # Apply square root of weight\n",
    "        \n",
    "        # Concatenate all weighted scores\n",
    "        combined_scores = np.hstack(weighted_scores)\n",
    "        \n",
    "        # Step 3: Apply global PCA\n",
    "        self.global_pca = PCA(n_components=self.n_components)\n",
    "        mfa_scores = self.global_pca.fit_transform(combined_scores)\n",
    "        \n",
    "        return mfa_scores\n",
    "\n",
    "# Apply MFA\n",
    "mfa = SimpleMFA(groups, n_components=10)\n",
    "X_mfa = mfa.fit_transform(X)\n",
    "\n",
    "print(f\"MFA Results:\")\n",
    "print(f\"MFA transformed shape: {X_mfa.shape}\")\n",
    "print(f\"Explained variance ratio: {mfa.global_pca.explained_variance_ratio_}\")\n",
    "print(f\"Cumulative explained variance: {np.cumsum(mfa.global_pca.explained_variance_ratio_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison and Conclusions\n",
    "\n",
    "Let's compare PCA and MFA results and draw conclusions about their effectiveness for this tumor diagnosis dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare PCA and MFA visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "if y is not None:\n",
    "    unique_labels = np.unique(y)\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    # PCA plot\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = y == label\n",
    "        axes[0].scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                       c=[colors[i]], label=f'{label}', alpha=0.6, s=50)\n",
    "    \n",
    "    axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    axes[0].set_title('PCA: First Two Principal Components')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # MFA plot\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = y == label\n",
    "        axes[1].scatter(X_mfa[mask, 0], X_mfa[mask, 1], \n",
    "                       c=[colors[i]], label=f'{label}', alpha=0.6, s=50)\n",
    "    \n",
    "    axes[1].set_xlabel(f'MFA1 ({mfa.global_pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    axes[1].set_ylabel(f'MFA2 ({mfa.global_pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    axes[1].set_title('MFA: First Two Components')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "else:\n",
    "    # No target variable\n",
    "    axes[0].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6)\n",
    "    axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    axes[0].set_title('PCA: First Two Principal Components')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].scatter(X_mfa[:, 0], X_mfa[:, 1], alpha=0.6)\n",
    "    axes[1].set_xlabel(f'MFA1 ({mfa.global_pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    axes[1].set_ylabel(f'MFA2 ({mfa.global_pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    axes[1].set_title('MFA: First Two Components')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative comparison\n",
    "print(\"=== PCA vs MFA Comparison ===\")\n",
    "print(f\"Variance explained by first 2 components:\")\n",
    "print(f\"PCA: {np.sum(pca.explained_variance_ratio_[:2]):.3f} ({pca.explained_variance_ratio_[0]:.3f} + {pca.explained_variance_ratio_[1]:.3f})\")\n",
    "print(f\"MFA: {np.sum(mfa.global_pca.explained_variance_ratio_[:2]):.3f} ({mfa.global_pca.explained_variance_ratio_[0]:.3f} + {mfa.global_pca.explained_variance_ratio_[1]:.3f})\")\n",
    "\n",
    "print(f\"\\nComponents needed for 80% variance:\")\n",
    "pca_80 = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.8) + 1\n",
    "mfa_80 = np.argmax(np.cumsum(mfa.global_pca.explained_variance_ratio_) >= 0.8) + 1\n",
    "print(f\"PCA: {pca_80} components\")\n",
    "print(f\"MFA: {mfa_80} components\")\n",
    "\n",
    "print(f\"Components needed for 95% variance:\")\n",
    "pca_95 = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95) + 1\n",
    "mfa_95 = np.argmax(np.cumsum(mfa.global_pca.explained_variance_ratio_) >= 0.95) + 1\n",
    "print(f\"PCA: {pca_95} components\")\n",
    "print(f\"MFA: {mfa_95} components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings and Conclusions\n",
    "\n",
    "**PCA Results:**\n",
    "- Traditional PCA provides a straightforward dimensionality reduction\n",
    "- All features are treated equally in the analysis\n",
    "- Effective for general-purpose dimensionality reduction\n",
    "- Good baseline for comparison\n",
    "\n",
    "**MFA Results:**\n",
    "- Takes into account the grouped structure of features\n",
    "- Balances contribution from different feature groups\n",
    "- May provide better interpretation when features have natural groupings\n",
    "- More complex but potentially more informative for structured data\n",
    "\n",
    "**When to Choose Each Method:**\n",
    "\n",
    "**Use PCA when:**\n",
    "- Features don't have natural groupings\n",
    "- You want a simple, straightforward analysis\n",
    "- All features are of similar importance\n",
    "- You need a quick exploratory analysis\n",
    "\n",
    "**Use MFA when:**\n",
    "- Features can be logically grouped\n",
    "- Some feature groups have many more variables than others\n",
    "- You want to prevent one group from dominating the analysis\n",
    "- You're working with mixed-type data or structured datasets\n",
    "\n",
    "**Practical Recommendations:**\n",
    "1. Always start with exploratory data analysis to understand your feature structure\n",
    "2. Consider the interpretability requirements of your analysis\n",
    "3. Validate results with domain knowledge\n",
    "4. Consider the computational complexity vs. benefit trade-off\n",
    "5. Use cross-validation or other methods to assess the stability of your results\n",
    "\n",
    "Both PCA and MFA are powerful techniques for dimensionality reduction and data visualization. The choice between them depends on your specific data structure and analysis goals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}